"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[8176],{1691(n,e,r){r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3-decision-making","title":"Chapter 4 - AI-Driven Decision Making","description":"Reinforcement Learning for Robot Behaviors","source":"@site/docs/module-3-decision-making.md","sourceDirName":".","slug":"/module-3-decision-making","permalink":"/ur/docs/module-3-decision-making","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-decision-making.md","tags":[],"version":"current","frontMatter":{"id":"module-3-decision-making","title":"Chapter 4 - AI-Driven Decision Making","sidebar_label":"Chapter 4 - AI-Driven Decision Making"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3 - Control Systems and Motion Planning","permalink":"/ur/docs/module-3-control-systems"},"next":{"title":"Chapter 5 - Isaac Sim Integration and Testing","permalink":"/ur/docs/module-3-sim-integration"}}');var s=r(4848),o=r(8453);const t={id:"module-3-decision-making",title:"Chapter 4 - AI-Driven Decision Making",sidebar_label:"Chapter 4 - AI-Driven Decision Making"},a="Chapter 4: AI-Driven Decision Making",l={},c=[{value:"Reinforcement Learning for Robot Behaviors",id:"reinforcement-learning-for-robot-behaviors",level:2},{value:"Reinforcement Learning Concepts in Isaac",id:"reinforcement-learning-concepts-in-isaac",level:3},{value:"Isaac Gym for RL Training",id:"isaac-gym-for-rl-training",level:3},{value:"Markov Decision Processes in Robotics",id:"markov-decision-processes-in-robotics",level:3},{value:"Neural Networks for Decision Making",id:"neural-networks-for-decision-making",level:2},{value:"Neural Network Integration in Isaac",id:"neural-network-integration-in-isaac",level:3},{value:"Deep Learning for Robot Decision Making",id:"deep-learning-for-robot-decision-making",level:3},{value:"Convolutional and Recurrent Networks",id:"convolutional-and-recurrent-networks",level:3},{value:"Task Planning and Execution",id:"task-planning-and-execution",level:2},{value:"Task Planning in Isaac Context",id:"task-planning-in-isaac-context",level:3},{value:"Hierarchical Task Networks",id:"hierarchical-task-networks",level:3},{value:"Symbolic Planning Approaches",id:"symbolic-planning-approaches",level:3},{value:"Learning from Demonstration",id:"learning-from-demonstration",level:2},{value:"Imitation Learning in Isaac",id:"imitation-learning-in-isaac",level:3},{value:"Behavior Cloning Techniques",id:"behavior-cloning-techniques",level:3},{value:"Inverse Reinforcement Learning",id:"inverse-reinforcement-learning",level:3},{value:"Multi-Modal AI for Complex Tasks",id:"multi-modal-ai-for-complex-tasks",level:2},{value:"Multi-modal AI Integration",id:"multi-modal-ai-integration",level:3},{value:"Combining Vision, Language, and Action",id:"combining-vision-language-and-action",level:3},{value:"Attention Mechanisms for Multi-modal Fusion",id:"attention-mechanisms-for-multi-modal-fusion",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-4-ai-driven-decision-making",children:"Chapter 4: AI-Driven Decision Making"})}),"\n",(0,s.jsx)(e.h2,{id:"reinforcement-learning-for-robot-behaviors",children:"Reinforcement Learning for Robot Behaviors"}),"\n",(0,s.jsx)(e.p,{children:"Reinforcement Learning (RL) is a powerful approach for enabling robots to learn complex behaviors through interaction with their environment. The NVIDIA Isaac platform provides comprehensive tools for implementing RL in robotic applications."}),"\n",(0,s.jsx)(e.h3,{id:"reinforcement-learning-concepts-in-isaac",children:"Reinforcement Learning Concepts in Isaac"}),"\n",(0,s.jsx)(e.p,{children:"Isaac's reinforcement learning framework includes:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"GPU-Accelerated Training"}),": Parallel simulation for faster learning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Isaac Gym"}),": Specialized environment for RL training"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Curriculum Learning"}),": Progressive training from simple to complex tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Transfer Learning"}),": Transferring policies from simulation to real robots"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"isaac-gym-for-rl-training",children:"Isaac Gym for RL Training"}),"\n",(0,s.jsx)(e.p,{children:"Isaac Gym provides a specialized environment for reinforcement learning with:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parallel Simulation"}),": Train multiple agents simultaneously"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Physics-based Simulation"}),": Accurate simulation of robot dynamics"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Flexible Reward Design"}),": Easy-to-define reward functions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Domain Randomization"}),": Training robust policies"]}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# Example: Isaac Gym environment for humanoid robot\r\nimport torch\r\nimport numpy as np\r\nimport omni\r\nfrom omni.isaac.gym.tasks import BaseTask\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\nfrom omni.isaac.core.articulations import ArticulationView\r\nfrom omni.isaac.core.utils.prims import get_prim_at_path\r\nfrom omni.isaac.core.utils.torch.maths import torch_rand_float\r\nfrom omni.isaac.core.utils.torch.rotations import *\r\nfrom pxr import PhysxSchema\r\n\r\nclass IsaacHumanoidTask(BaseTask):\r\n    def __init__(self, name, offset=None):\r\n        # Set up task properties\r\n        self._num_envs = 1024  # Number of parallel environments\r\n        self._env_spacing = 2.5\r\n        self._action_space = 12  # 12 DOF for humanoid\r\n        self._obs_space = 41  # Observation space size\r\n\r\n        # Call parent constructor\r\n        super().__init__(name=name, offset=offset)\r\n\r\n    def set_up_scene(self, scene):\r\n        # Add humanoid robot to each environment\r\n        for i in range(self._num_envs):\r\n            add_reference_to_stage(\r\n                usd_path="/Isaac/Robots/Humanoid/humanoid.usd",\r\n                prim_path=f"/World/envs/env_{i}/Humanoid"\r\n            )\r\n\r\n        super().set_up_scene(scene)\r\n\r\n        # Create articulation view for the humanoid robots\r\n        self._humanoids = ArticulationView(\r\n            prim_paths_expr="/World/envs/.*/Humanoid",\r\n            name="humanoid_view",\r\n            reset_xform_properties=False,\r\n        )\r\n        scene.add(self._humanoids)\r\n\r\n    def get_observations(self):\r\n        # Get observations for all environments\r\n        obs = torch.zeros((self._num_envs, self._obs_space), device=self._device)\r\n        # Implement observation gathering logic\r\n        return obs\r\n\r\n    def pre_physics_step(self, actions):\r\n        # Process actions before physics simulation\r\n        actions = torch.clamp(actions, -1.0, 1.0)\r\n        # Apply actions to humanoid robots\r\n        self._humanoids.set_joint_position_targets(actions)\r\n\r\n    def get_rewards(self):\r\n        # Calculate rewards for each environment\r\n        rewards = torch.zeros(self._num_envs, device=self._device)\r\n        # Implement reward calculation logic\r\n        return rewards\r\n\r\n    def get_dones(self):\r\n        # Check if episodes are done\r\n        dones = torch.zeros(self._num_envs, device=self._device, dtype=torch.bool)\r\n        # Implement termination conditions\r\n        return dones\r\n\r\n    def reset_idx(self, env_ids):\r\n        # Reset specific environments\r\n        if len(env_ids) == 0:\r\n            return\r\n        # Reset humanoid positions and states\r\n        pass\n'})}),"\n",(0,s.jsx)(e.h3,{id:"markov-decision-processes-in-robotics",children:"Markov Decision Processes in Robotics"}),"\n",(0,s.jsx)(e.p,{children:"In robotics, Markov Decision Processes (MDPs) provide a mathematical framework for decision-making:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"State Space"}),": Robot configuration, environment state"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action Space"}),": Joint commands, navigation commands"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Reward Function"}),": Task completion, efficiency, safety"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Transition Model"}),": Robot dynamics and environment interactions"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"neural-networks-for-decision-making",children:"Neural Networks for Decision Making"}),"\n",(0,s.jsx)(e.p,{children:"Deep neural networks form the foundation of modern AI-driven decision making in robotics."}),"\n",(0,s.jsx)(e.h3,{id:"neural-network-integration-in-isaac",children:"Neural Network Integration in Isaac"}),"\n",(0,s.jsx)(e.p,{children:"Isaac provides seamless integration with neural networks through:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"TensorRT Optimization"}),": Optimized inference for deployment"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"CUDA Acceleration"}),": GPU-accelerated neural network execution"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Model Conversion"}),": Easy conversion from training to deployment"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Real-time Inference"}),": Optimized for real-time robotics applications"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"deep-learning-for-robot-decision-making",children:"Deep Learning for Robot Decision Making"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Example: Neural network for robot decision making\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import JointState, Image, Imu\r\nfrom geometry_msgs.msg import Twist\r\nfrom std_msgs.msg import Float64MultiArray\r\nimport numpy as np\r\n\r\nclass RobotDecisionNetwork(nn.Module):\r\n    def __init__(self, input_size, hidden_size, output_size):\r\n        super(RobotDecisionNetwork, self).__init__()\r\n\r\n        # Perception layers\r\n        self.perception_net = nn.Sequential(\r\n            nn.Linear(input_size, hidden_size),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_size, hidden_size),\r\n            nn.ReLU()\r\n        )\r\n\r\n        # Decision making layers\r\n        self.decision_net = nn.Sequential(\r\n            nn.Linear(hidden_size, hidden_size),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_size, output_size)\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.perception_net(x)\r\n        x = self.decision_net(x)\r\n        return x\r\n\r\nclass IsaacDecisionMaker(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_decision_maker')\r\n\r\n        # Neural network model\r\n        self.model = RobotDecisionNetwork(\r\n            input_size=50,  # Example: joint states, IMU, camera features\r\n            hidden_size=256,\r\n            output_size=12   # Example: joint commands\r\n        )\r\n\r\n        # Load pre-trained model\r\n        self.load_model()\r\n\r\n        # Subscribers\r\n        self.joint_sub = self.create_subscription(\r\n            JointState, '/joint_states', self.joint_callback, 10\r\n        )\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, '/imu/data', self.imu_callback, 10\r\n        )\r\n        self.camera_sub = self.create_subscription(\r\n            Image, '/camera/image_raw', self.camera_callback, 10\r\n        )\r\n\r\n        # Publisher for decisions\r\n        self.command_pub = self.create_publisher(\r\n            Float64MultiArray, '/robot_commands', 10\r\n        )\r\n\r\n        # Timer for decision making\r\n        self.timer = self.create_timer(0.05, self.make_decision)  # 20 Hz\r\n\r\n        # Robot state\r\n        self.joint_state = None\r\n        self.imu_data = None\r\n        self.camera_data = None\r\n\r\n    def load_model(self):\r\n        # Load pre-trained neural network\r\n        # This would load a model trained in Isaac Gym\r\n        pass\r\n\r\n    def joint_callback(self, msg):\r\n        self.joint_state = msg\r\n\r\n    def imu_callback(self, msg):\r\n        self.imu_data = msg\r\n\r\n    def camera_callback(self, msg):\r\n        self.camera_data = msg\r\n\r\n    def make_decision(self):\r\n        # Combine sensor data into input vector\r\n        input_vector = self.combine_sensor_data()\r\n\r\n        # Make decision using neural network\r\n        with torch.no_grad():\r\n            commands = self.model(input_vector)\r\n\r\n        # Publish commands\r\n        cmd_msg = Float64MultiArray()\r\n        cmd_msg.data = commands.numpy().tolist()\r\n        self.command_pub.publish(cmd_msg)\r\n\r\n    def combine_sensor_data(self):\r\n        # Combine all sensor data into a single input vector\r\n        # This would include joint states, IMU, camera features, etc.\r\n        pass\n"})}),"\n",(0,s.jsx)(e.h3,{id:"convolutional-and-recurrent-networks",children:"Convolutional and Recurrent Networks"}),"\n",(0,s.jsx)(e.p,{children:"Isaac supports various neural network architectures:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"CNNs"}),": For processing visual and sensor data"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"RNNs"}),": For temporal sequence processing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"LSTMs"}),": For long-term memory in decision making"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Transformers"}),": For attention-based decision making"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"task-planning-and-execution",children:"Task Planning and Execution"}),"\n",(0,s.jsx)(e.p,{children:"Task planning involves breaking down complex goals into executable actions that the robot can perform."}),"\n",(0,s.jsx)(e.h3,{id:"task-planning-in-isaac-context",children:"Task Planning in Isaac Context"}),"\n",(0,s.jsx)(e.p,{children:"Isaac's task planning capabilities include:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Hierarchical Task Networks"}),": Decomposing complex tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Temporal Planning"}),": Considering time constraints"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Resource Management"}),": Managing robot resources efficiently"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-agent Coordination"}),": Planning for multiple robots"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"hierarchical-task-networks",children:"Hierarchical Task Networks"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# Example: Hierarchical task planning\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom action_msgs.msg import GoalStatus\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom sensor_msgs.msg import JointState\r\n\r\nclass IsaacTaskPlanner(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_task_planner\')\r\n\r\n        # Publishers and subscribers\r\n        self.task_pub = self.create_publisher(String, \'/current_task\', 10)\r\n        self.goal_pub = self.create_publisher(PoseStamped, \'/move_base_simple/goal\', 10)\r\n        self.joint_pub = self.create_publisher(JointState, \'/joint_commands\', 10)\r\n\r\n        # Task queue\r\n        self.task_queue = []\r\n        self.current_task = None\r\n\r\n        # Timer for task execution\r\n        self.timer = self.create_timer(0.1, self.execute_task)\r\n\r\n    def plan_task_sequence(self, goal):\r\n        # Plan sequence of tasks to achieve goal\r\n        # Example: "Navigate to object, grasp object, place object"\r\n        tasks = self.decompose_goal(goal)\r\n        self.task_queue.extend(tasks)\r\n\r\n    def decompose_goal(self, goal):\r\n        # Decompose high-level goal into subtasks\r\n        if goal.task_type == "fetch_object":\r\n            return [\r\n                {"type": "navigate", "target": goal.location},\r\n                {"type": "perceive", "target": goal.object},\r\n                {"type": "grasp", "target": goal.object},\r\n                {"type": "navigate", "target": goal.destination},\r\n                {"type": "place", "target": goal.object}\r\n            ]\r\n        return []\r\n\r\n    def execute_task(self):\r\n        if not self.task_queue:\r\n            return\r\n\r\n        if self.current_task is None:\r\n            self.current_task = self.task_queue.pop(0)\r\n            self.begin_task_execution(self.current_task)\r\n\r\n        # Check if current task is complete\r\n        if self.is_task_complete(self.current_task):\r\n            self.current_task = None\r\n\r\n    def begin_task_execution(self, task):\r\n        # Begin executing a specific task\r\n        if task["type"] == "navigate":\r\n            self.execute_navigation_task(task)\r\n        elif task["type"] == "grasp":\r\n            self.execute_grasp_task(task)\r\n        elif task["type"] == "perceive":\r\n            self.execute_perception_task(task)\r\n\r\n    def is_task_complete(self, task):\r\n        # Check if task is complete\r\n        # This would check task-specific completion criteria\r\n        pass\n'})}),"\n",(0,s.jsx)(e.h3,{id:"symbolic-planning-approaches",children:"Symbolic Planning Approaches"}),"\n",(0,s.jsx)(e.p,{children:"Isaac supports symbolic planning through:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"STRIPS"}),": Classical planning with preconditions and effects"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"PDDL"}),": Planning Domain Definition Language"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Behavior Trees"}),": Hierarchical task representation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Finite State Machines"}),": State-based task execution"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"learning-from-demonstration",children:"Learning from Demonstration"}),"\n",(0,s.jsx)(e.p,{children:"Learning from Demonstration (LfD) allows robots to learn behaviors by observing human demonstrations."}),"\n",(0,s.jsx)(e.h3,{id:"imitation-learning-in-isaac",children:"Imitation Learning in Isaac"}),"\n",(0,s.jsx)(e.p,{children:"Isaac's imitation learning capabilities include:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Behavior Cloning"}),": Learning from expert demonstrations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Inverse Reinforcement Learning"}),": Learning reward functions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Dagger Algorithm"}),": Interactive learning approach"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Kinesthetic Teaching"}),": Physical guidance of robot movements"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"behavior-cloning-techniques",children:"Behavior Cloning Techniques"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Example: Learning from demonstration\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import JointState\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom std_msgs.msg import Bool\r\n\r\nclass IsaacLearningFromDemo(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_learning_from_demo')\r\n\r\n        # Demonstration buffer\r\n        self.demo_buffer = []\r\n        self.current_demo = []\r\n        self.is_recording = False\r\n\r\n        # Neural network for behavior cloning\r\n        self.behavior_net = nn.Sequential(\r\n            nn.Linear(24, 64),  # Input: 12 joint positions + 12 joint velocities\r\n            nn.ReLU(),\r\n            nn.Linear(64, 64),\r\n            nn.ReLU(),\r\n            nn.Linear(64, 12)   # Output: 12 joint commands\r\n        )\r\n\r\n        # Subscribers and publishers\r\n        self.joint_sub = self.create_subscription(\r\n            JointState, '/joint_states', self.joint_callback, 10\r\n        )\r\n        self.demo_control_sub = self.create_subscription(\r\n            Bool, '/start_demo_recording', self.demo_control_callback, 10\r\n        )\r\n        self.command_pub = self.create_publisher(\r\n            JointState, '/cloned_commands', 10\r\n        )\r\n\r\n        # Training timer\r\n        self.train_timer = self.create_timer(1.0, self.train_network)\r\n\r\n    def joint_callback(self, msg):\r\n        if self.is_recording:\r\n            # Store demonstration data\r\n            demo_entry = {\r\n                'state': msg.position + msg.velocity,  # State is position + velocity\r\n                'action': msg.effort  # Action is the demonstrated command\r\n            }\r\n            self.current_demo.append(demo_entry)\r\n\r\n    def demo_control_callback(self, msg):\r\n        if msg.data:  # Start recording\r\n            self.start_recording()\r\n        else:  # Stop recording\r\n            self.stop_recording()\r\n\r\n    def start_recording(self):\r\n        self.is_recording = True\r\n        self.current_demo = []\r\n        self.get_logger().info(\"Started recording demonstration\")\r\n\r\n    def stop_recording(self):\r\n        self.is_recording = False\r\n        if self.current_demo:\r\n            self.demo_buffer.append(self.current_demo)\r\n            self.get_logger().info(f\"Recorded demonstration: {len(self.current_demo)} steps\")\r\n        self.current_demo = []\r\n\r\n    def train_network(self):\r\n        if len(self.demo_buffer) < 1:\r\n            return\r\n\r\n        # Prepare training data\r\n        states = []\r\n        actions = []\r\n\r\n        for demo in self.demo_buffer:\r\n            for entry in demo:\r\n                states.append(entry['state'])\r\n                actions.append(entry['action'])\r\n\r\n        if len(states) == 0:\r\n            return\r\n\r\n        # Convert to tensors\r\n        state_tensor = torch.tensor(states, dtype=torch.float32)\r\n        action_tensor = torch.tensor(actions, dtype=torch.float32)\r\n\r\n        # Train the network\r\n        criterion = nn.MSELoss()\r\n        optimizer = torch.optim.Adam(self.behavior_net.parameters())\r\n\r\n        optimizer.zero_grad()\r\n        predicted_actions = self.behavior_net(state_tensor)\r\n        loss = criterion(predicted_actions, action_tensor)\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        self.get_logger().info(f\"Training loss: {loss.item():.4f}\")\n"})}),"\n",(0,s.jsx)(e.h3,{id:"inverse-reinforcement-learning",children:"Inverse Reinforcement Learning"}),"\n",(0,s.jsx)(e.p,{children:"Inverse Reinforcement Learning (IRL) learns the reward function from demonstrations:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Maximum Entropy IRL"}),": Learning reward functions that explain expert behavior"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Guided Cost Learning"}),": Learning cost functions from demonstrations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Adversarial IRL"}),": Using adversarial training to learn reward functions"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"multi-modal-ai-for-complex-tasks",children:"Multi-Modal AI for Complex Tasks"}),"\n",(0,s.jsx)(e.p,{children:"Multi-modal AI combines different types of information (vision, language, action) to enable more sophisticated robot capabilities."}),"\n",(0,s.jsx)(e.h3,{id:"multi-modal-ai-integration",children:"Multi-modal AI Integration"}),"\n",(0,s.jsx)(e.p,{children:"Isaac's multi-modal AI capabilities include:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision-Language Models"}),": Understanding natural language commands with visual context"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sensor Fusion"}),": Combining multiple sensor modalities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cross-modal Attention"}),": Attention mechanisms across modalities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multimodal Decision Making"}),": Decisions based on multiple input types"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"combining-vision-language-and-action",children:"Combining Vision, Language, and Action"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Example: Multi-modal AI system\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, JointState\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped\r\n\r\nclass IsaacMultiModalAI(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_multi_modal_ai')\r\n\r\n        # Multi-modal neural network\r\n        self.vision_encoder = nn.Sequential(\r\n            nn.Conv2d(3, 32, 8, stride=4),\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 64, 4, stride=2),\r\n            nn.ReLU(),\r\n            nn.Conv2d(64, 64, 3, stride=1),\r\n            nn.ReLU(),\r\n            nn.Flatten(),\r\n            nn.Linear(256, 256)\r\n        )\r\n\r\n        self.language_encoder = nn.LSTM(300, 256)  # Assuming 300-dim word embeddings\r\n\r\n        self.fusion_network = nn.Sequential(\r\n            nn.Linear(512, 512),  # 256 vision + 256 language\r\n            nn.ReLU(),\r\n            nn.Linear(512, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 12)  # 12 DOF output\r\n        )\r\n\r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/image_raw', self.image_callback, 10\r\n        )\r\n        self.command_sub = self.create_subscription(\r\n            String, '/natural_language_command', self.command_callback, 10\r\n        )\r\n\r\n        # Publisher\r\n        self.action_pub = self.create_publisher(\r\n            JointState, '/multi_modal_action', 10\r\n        )\r\n\r\n        # Data buffers\r\n        self.current_image = None\r\n        self.current_command = None\r\n\r\n        # Timer for multi-modal processing\r\n        self.timer = self.create_timer(0.1, self.process_multimodal_input)\r\n\r\n    def image_callback(self, msg):\r\n        # Process camera image\r\n        self.current_image = self.process_image(msg)\r\n\r\n    def command_callback(self, msg):\r\n        # Process natural language command\r\n        self.current_command = self.process_language_command(msg.data)\r\n\r\n    def process_image(self, image_msg):\r\n        # Convert ROS image to tensor\r\n        # This is a simplified example\r\n        pass\r\n\r\n    def process_language_command(self, command_str):\r\n        # Convert natural language to embedding\r\n        # This would use a pre-trained language model\r\n        pass\r\n\r\n    def process_multimodal_input(self):\r\n        if self.current_image is None or self.current_command is None:\r\n            return\r\n\r\n        # Encode visual input\r\n        vision_features = self.vision_encoder(self.current_image)\r\n\r\n        # Encode language input\r\n        lang_features = self.language_encoder(self.current_command)[0][-1]  # Last hidden state\r\n\r\n        # Fuse modalities\r\n        fused_features = torch.cat([vision_features, lang_features], dim=1)\r\n\r\n        # Generate action\r\n        action = self.fusion_network(fused_features)\r\n\r\n        # Publish action\r\n        joint_cmd = JointState()\r\n        joint_cmd.position = action.detach().numpy().tolist()\r\n        self.action_pub.publish(joint_cmd)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"attention-mechanisms-for-multi-modal-fusion",children:"Attention Mechanisms for Multi-modal Fusion"}),"\n",(0,s.jsx)(e.p,{children:"Multi-modal attention mechanisms help the robot focus on relevant information:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cross-Modal Attention"}),": Attending to relevant parts across modalities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Self-Attention"}),": Attending to relevant parts within a modality"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Spatial Attention"}),": Focusing on relevant spatial regions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Temporal Attention"}),": Focusing on relevant time steps"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(e.p,{children:"Isaac's AI-driven decision making capabilities provide a comprehensive framework for enabling intelligent robot behavior. From reinforcement learning for complex skill acquisition to multi-modal AI for sophisticated task understanding, the platform provides the tools necessary for creating truly intelligent robotic systems. The combination of GPU acceleration, advanced neural network architectures, and specialized robotics algorithms enables humanoid robots to learn, adapt, and make intelligent decisions in complex real-world environments."})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453(n,e,r){r.d(e,{R:()=>t,x:()=>a});var i=r(6540);const s={},o=i.createContext(s);function t(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:t(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);